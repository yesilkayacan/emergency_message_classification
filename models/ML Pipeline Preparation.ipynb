{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yesil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yesil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yesil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yesil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, precision_score, recall_score\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterMessageTable', engine)\n",
    "X = df['message'].values\n",
    "Y = df[df.columns[4:]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    text = re.sub(r\"[a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    clean_tokens = [lemmatizer.lemmatize(tok).strip() for tok in tokens if tok not in stop_words]\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_forest = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at...\n",
       "                                                                        ccp_alpha=0.0,\n",
       "                                                                        class_weight=None,\n",
       "                                                                        criterion='gini',\n",
       "                                                                        max_depth=None,\n",
       "                                                                        max_features='auto',\n",
       "                                                                        max_leaf_nodes=None,\n",
       "                                                                        max_samples=None,\n",
       "                                                                        min_impurity_decrease=0.0,\n",
       "                                                                        min_impurity_split=None,\n",
       "                                                                        min_samples_leaf=1,\n",
       "                                                                        min_samples_split=2,\n",
       "                                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                                        n_estimators=100,\n",
       "                                                                        n_jobs=None,\n",
       "                                                                        oob_score=False,\n",
       "                                                                        random_state=None,\n",
       "                                                                        verbose=0,\n",
       "                                                                        warm_start=False),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.07      0.12      1261\n",
      "           1       0.76      0.97      0.85      3929\n",
      "           2       0.17      0.02      0.04        46\n",
      "\n",
      "    accuracy                           0.75      5236\n",
      "   macro avg       0.47      0.36      0.34      5236\n",
      "weighted avg       0.69      0.75      0.67      5236\n",
      "\n",
      "request :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.91      4359\n",
      "           1       0.46      0.09      0.15       877\n",
      "\n",
      "    accuracy                           0.83      5236\n",
      "   macro avg       0.65      0.53      0.53      5236\n",
      "weighted avg       0.78      0.83      0.78      5236\n",
      "\n",
      "offer :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5220\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           1.00      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      1.00      0.99      5236\n",
      "\n",
      "aid_related :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.80      0.71      3088\n",
      "           1       0.54      0.33      0.41      2148\n",
      "\n",
      "    accuracy                           0.61      5236\n",
      "   macro avg       0.59      0.57      0.56      5236\n",
      "weighted avg       0.60      0.61      0.59      5236\n",
      "\n",
      "medical_help :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      4847\n",
      "           1       0.28      0.03      0.05       389\n",
      "\n",
      "    accuracy                           0.92      5236\n",
      "   macro avg       0.60      0.51      0.50      5236\n",
      "weighted avg       0.88      0.92      0.89      5236\n",
      "\n",
      "medical_products :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      4970\n",
      "           1       0.17      0.02      0.03       266\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.56      0.51      0.50      5236\n",
      "weighted avg       0.91      0.95      0.92      5236\n",
      "\n",
      "search_and_rescue :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5106\n",
      "           1       0.00      0.00      0.00       130\n",
      "\n",
      "    accuracy                           0.97      5236\n",
      "   macro avg       0.49      0.50      0.49      5236\n",
      "weighted avg       0.95      0.97      0.96      5236\n",
      "\n",
      "security :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5141\n",
      "           1       0.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.96      0.98      0.97      5236\n",
      "\n",
      "military :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      5062\n",
      "           1       0.11      0.01      0.01       174\n",
      "\n",
      "    accuracy                           0.97      5236\n",
      "   macro avg       0.54      0.50      0.50      5236\n",
      "weighted avg       0.94      0.97      0.95      5236\n",
      "\n",
      "child_alone :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5236\n",
      "\n",
      "    accuracy                           1.00      5236\n",
      "   macro avg       1.00      1.00      1.00      5236\n",
      "weighted avg       1.00      1.00      1.00      5236\n",
      "\n",
      "water :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      4876\n",
      "           1       0.16      0.02      0.03       360\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.54      0.51      0.50      5236\n",
      "weighted avg       0.88      0.93      0.90      5236\n",
      "\n",
      "food :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94      4660\n",
      "           1       0.35      0.05      0.09       576\n",
      "\n",
      "    accuracy                           0.89      5236\n",
      "   macro avg       0.62      0.52      0.51      5236\n",
      "weighted avg       0.83      0.89      0.84      5236\n",
      "\n",
      "shelter :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.99      0.95      4769\n",
      "           1       0.27      0.03      0.05       467\n",
      "\n",
      "    accuracy                           0.91      5236\n",
      "   macro avg       0.59      0.51      0.50      5236\n",
      "weighted avg       0.86      0.91      0.87      5236\n",
      "\n",
      "clothing :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5151\n",
      "           1       0.17      0.01      0.02        85\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.58      0.51      0.51      5236\n",
      "weighted avg       0.97      0.98      0.98      5236\n",
      "\n",
      "money :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5102\n",
      "           1       0.26      0.04      0.07       134\n",
      "\n",
      "    accuracy                           0.97      5236\n",
      "   macro avg       0.62      0.52      0.53      5236\n",
      "weighted avg       0.96      0.97      0.96      5236\n",
      "\n",
      "missing_people :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5177\n",
      "           1       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.98      0.99      0.98      5236\n",
      "\n",
      "refugees :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      5056\n",
      "           1       0.07      0.01      0.01       180\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.52      0.50      0.50      5236\n",
      "weighted avg       0.93      0.96      0.95      5236\n",
      "\n",
      "death :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5003\n",
      "           1       0.36      0.02      0.04       233\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.66      0.51      0.51      5236\n",
      "weighted avg       0.93      0.95      0.94      5236\n",
      "\n",
      "other_aid :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.93      4569\n",
      "           1       0.23      0.02      0.03       667\n",
      "\n",
      "    accuracy                           0.87      5236\n",
      "   macro avg       0.55      0.50      0.48      5236\n",
      "weighted avg       0.79      0.87      0.81      5236\n",
      "\n",
      "infrastructure_related :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      4889\n",
      "           1       0.17      0.01      0.02       347\n",
      "\n",
      "    accuracy                           0.93      5236\n",
      "   macro avg       0.55      0.50      0.49      5236\n",
      "weighted avg       0.88      0.93      0.90      5236\n",
      "\n",
      "transport :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5013\n",
      "           1       0.18      0.01      0.02       223\n",
      "\n",
      "    accuracy                           0.96      5236\n",
      "   macro avg       0.57      0.50      0.50      5236\n",
      "weighted avg       0.92      0.96      0.94      5236\n",
      "\n",
      "buildings :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yesil\\anaconda3\\envs\\Udacity_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4987\n",
      "           1       0.12      0.01      0.02       249\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.53      0.50      0.50      5236\n",
      "weighted avg       0.91      0.95      0.93      5236\n",
      "\n",
      "electricity :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5131\n",
      "           1       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.49      0.50      0.49      5236\n",
      "weighted avg       0.96      0.98      0.97      5236\n",
      "\n",
      "tools :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5208\n",
      "           1       0.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "hospitals :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5185\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.98      0.99      0.99      5236\n",
      "\n",
      "shops :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5210\n",
      "           1       0.00      0.00      0.00        26\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.99      0.99      0.99      5236\n",
      "\n",
      "aid_centers :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      5166\n",
      "           1       0.00      0.00      0.00        70\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.49      0.50      0.50      5236\n",
      "weighted avg       0.97      0.99      0.98      5236\n",
      "\n",
      "other_infrastructure :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5001\n",
      "           1       0.25      0.01      0.02       235\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.60      0.50      0.50      5236\n",
      "weighted avg       0.92      0.95      0.93      5236\n",
      "\n",
      "weather_related :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83      3780\n",
      "           1       0.53      0.14      0.22      1456\n",
      "\n",
      "    accuracy                           0.73      5236\n",
      "   macro avg       0.63      0.54      0.52      5236\n",
      "weighted avg       0.68      0.73      0.66      5236\n",
      "\n",
      "floods :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      4801\n",
      "           1       0.43      0.03      0.06       435\n",
      "\n",
      "    accuracy                           0.92      5236\n",
      "   macro avg       0.68      0.51      0.51      5236\n",
      "weighted avg       0.88      0.92      0.88      5236\n",
      "\n",
      "storm :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.95      4781\n",
      "           1       0.46      0.13      0.20       455\n",
      "\n",
      "    accuracy                           0.91      5236\n",
      "   macro avg       0.69      0.56      0.57      5236\n",
      "weighted avg       0.88      0.91      0.89      5236\n",
      "\n",
      "fire :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      5185\n",
      "           1       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.99      5236\n",
      "   macro avg       0.50      0.50      0.50      5236\n",
      "weighted avg       0.98      0.99      0.99      5236\n",
      "\n",
      "earthquake :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.99      0.94      4700\n",
      "           1       0.38      0.06      0.10       536\n",
      "\n",
      "    accuracy                           0.89      5236\n",
      "   macro avg       0.64      0.52      0.52      5236\n",
      "weighted avg       0.85      0.89      0.86      5236\n",
      "\n",
      "cold :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5137\n",
      "           1       0.00      0.00      0.00        99\n",
      "\n",
      "    accuracy                           0.98      5236\n",
      "   macro avg       0.49      0.50      0.49      5236\n",
      "weighted avg       0.96      0.98      0.97      5236\n",
      "\n",
      "other_weather :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      4973\n",
      "           1       0.07      0.00      0.01       263\n",
      "\n",
      "    accuracy                           0.95      5236\n",
      "   macro avg       0.51      0.50      0.49      5236\n",
      "weighted avg       0.91      0.95      0.92      5236\n",
      "\n",
      "direct_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89      4260\n",
      "           1       0.48      0.11      0.18       976\n",
      "\n",
      "    accuracy                           0.81      5236\n",
      "   macro avg       0.65      0.54      0.54      5236\n",
      "weighted avg       0.76      0.81      0.76      5236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = df.columns[4:]\n",
    "\n",
    "for i in range(len(target_names)):\n",
    "    print(target_names[i], ':')\n",
    "    print(classification_report(y_test[:,i], y_pred[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__estimator__bootstrap', 'clf__estimator__ccp_alpha', 'clf__estimator__class_weight', 'clf__estimator__criterion', 'clf__estimator__max_depth', 'clf__estimator__max_features', 'clf__estimator__max_leaf_nodes', 'clf__estimator__max_samples', 'clf__estimator__min_impurity_decrease', 'clf__estimator__min_impurity_split', 'clf__estimator__min_samples_leaf', 'clf__estimator__min_samples_split', 'clf__estimator__min_weight_fraction_leaf', 'clf__estimator__n_estimators', 'clf__estimator__n_jobs', 'clf__estimator__oob_score', 'clf__estimator__random_state', 'clf__estimator__verbose', 'clf__estimator__warm_start', 'clf__estimator', 'clf__n_jobs'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_forest.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  21.2s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   21.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  20.6s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  21.9s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  21.4s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  21.0s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2), total=  38.5s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2), total=  37.9s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2), total=  37.2s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2), total=  37.8s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.5, vect__ngram_range=(1, 2), total=  37.1s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  25.6s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  24.3s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  25.1s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  24.8s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  25.4s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2), total=  39.9s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2), total=  39.3s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2), total=  39.6s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2), total=  38.7s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=0.75, vect__ngram_range=(1, 2), total=  39.5s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  31.0s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  30.2s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  30.4s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  31.2s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  32.8s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2), total=  43.8s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2), total=  39.7s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2), total=  39.8s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2), total=  40.3s\n",
      "[CV] clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=20, vect__max_df=1.0, vect__ngram_range=(1, 2), total=  39.5s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  31.5s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  31.2s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  31.1s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  31.1s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  31.1s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  41.7s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  41.2s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  41.3s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  41.3s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 1), total=  41.1s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.3min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.3min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.3min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.3min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  57.6s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  56.9s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  57.4s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  57.0s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 1), total=  57.7s\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.4min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.4min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.4min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.4min\n",
      "[CV] clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 46.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(...\n",
       "                                                                                               min_weight_fraction_leaf=0.0,\n",
       "                                                                                               n_estimators=100,\n",
       "                                                                                               n_jobs=None,\n",
       "                                                                                               oob_score=False,\n",
       "                                                                                               random_state=None,\n",
       "                                                                                               verbose=0,\n",
       "                                                                                               warm_start=False),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=1,\n",
       "             param_grid={'clf__estimator__n_estimators': [20, 50],\n",
       "                         'vect__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_forest = {\n",
    "            'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "            'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            'clf__estimator__n_estimators': [20, 50],\n",
    "            }\n",
    "\n",
    "cv_forest = GridSearchCV(pipeline_forest, param_grid=parameters_forest, n_jobs=1, verbose=2)\n",
    "cv_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__n_estimators': 50,\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_forest.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest = cv_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_performance(y_true, y_predicted):\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    for i in range(len(target_names)):\n",
    "        accuracy_list.append(balanced_accuracy_score(y_true[:,i], y_pred[:,i]))\n",
    "        precision_list.append(precision_score(y_true[:,i], y_pred[:,i], average='weighted'))\n",
    "        recall_list.append(recall_score(y_true[:,i], y_pred[:,i], average='weighted'))\n",
    "\n",
    "    return np.mean(accuracy_list), np.mean(precision_list), np.mean(recall_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "accuracy: 0.9258604957134369\n",
      "precisicion: 0.8963166766430276\n",
      "recall: 0.9258604957134369\n"
     ]
    }
   ],
   "source": [
    "acc_forest, precision_forest, recall_forest = model_performance(y_test, y_pred_forest)\n",
    "print('Model performance:\\naccuracy: {}\\nprecisicion: {}\\nrecall: {}'\n",
    "      .format(acc_forest, precision_forest, recall_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_kNN = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(KNeighborsClassifier()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vect', 'tfidf', 'clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'clf__estimator__algorithm', 'clf__estimator__leaf_size', 'clf__estimator__metric', 'clf__estimator__metric_params', 'clf__estimator__n_jobs', 'clf__estimator__n_neighbors', 'clf__estimator__p', 'clf__estimator__weights', 'clf__estimator', 'clf__n_jobs'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_kNN.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1), total=  59.6s\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   59.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.0min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.1min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.2min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.4min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.9min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.4min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=2, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.3min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.2min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.4min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.5min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.4min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.4min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.7min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=4, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.5min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 2.0min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.5, vect__ngram_range=(1, 2), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.6min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.7min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 1), total= 1.8min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 2.1min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.9min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 2.0min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 2.0min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=0.75, vect__ngram_range=(1, 2), total= 1.9min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.1min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.0min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.0min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 1), total= 2.1min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.3min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.2min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.1min\n",
      "[CV] clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_neighbors=6, vect__max_df=1.0, vect__ngram_range=(1, 2), total= 2.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 158.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        token_pattern='(...\n",
       "                                                                                             leaf_size=30,\n",
       "                                                                                             metric='minkowski',\n",
       "                                                                                             metric_params=None,\n",
       "                                                                                             n_jobs=None,\n",
       "                                                                                             n_neighbors=5,\n",
       "                                                                                             p=2,\n",
       "                                                                                             weights='uniform'),\n",
       "                                                              n_jobs=None))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=1,\n",
       "             param_grid={'clf__estimator__n_neighbors': [2, 4, 6],\n",
       "                         'vect__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_kNN = {\n",
    "            'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "            'vect__max_df': (0.5, 0.75, 1.0),\n",
    "            'clf__estimator__n_neighbors': [2, 4, 6],\n",
    "            }\n",
    "\n",
    "cv_kNN = GridSearchCV(pipeline_kNN, param_grid=parameters_kNN, n_jobs=1, verbose=2)\n",
    "cv_kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_kNN = cv_kNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "accuracy: 0.9258604957134369\n",
      "precisicion: 0.8963166766430276\n",
      "recall: 0.9258604957134369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yesil\\anaconda3\\envs\\Udacity_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc_kNN, precision_kNN, recall_kNN = model_performance(y_test, y_pred_kNN)\n",
    "print('Model performance:\\naccuracy: {}\\nprecisicion: {}\\nrecall: {}'\n",
    "      .format(acc_kNN, precision_kNN, recall_kNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_forest, f)\n",
    "\n",
    "with open('kNN.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_kNN, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
